{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. feed forward.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8dHgjOjJzCb"
      },
      "source": [
        "# Feed forward (순전파)\n",
        "\n",
        "Feed forward(순전파)는 Neural net(신경망) 학습의 초기 과정이다.   \n",
        "순전파를 통해 예측 값(pred_y)을 구하고 예측 값(y_pred)과 정답 값(test_y)의 loss를 구해 weight를 업데이트 하는 과정이 train(학습)이기 때문이다.  \n",
        "순전파는 미리 정의된 weight matrix와 bias에 입력 데이터를 넣어 dot prodcut (행렬곱)하여 예측 값을 계산한다.  \n",
        "\n",
        "CF) 학습이 모두 끝난 모델의 weight와 bias를 불러온 후, 새로운 데이터를 넣어 결과 값을 도출하는 과정을 inference라고 한다.\n",
        "\n",
        "구현하고자 하는 모델 구조는 아래와 같다.\n",
        "- W1 : 노드 3개\n",
        "- activate function : sigmoid\n",
        "- W2 : 노드 2개\n",
        "- activate function : sigmoid\n",
        "- W3 : 노드 2개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_0s4fENN1w2"
      },
      "source": [
        "## Numpy\n",
        "아래는 numpy를 사용해 구현한 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx8yB-cELHJ1",
        "outputId": "28d047b8-b41e-4d46-cadd-6aae2cbba247"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def build_model():\n",
        "    network = {}\n",
        "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
        "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
        "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
        "    network['b2'] = np.array([0.1, 0.2])\n",
        "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
        "    network['b3'] = np.array([0.1, 0.2])\n",
        "\n",
        "    return network\n",
        "\n",
        "def forward(network, x):\n",
        "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
        "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    z2 = sigmoid(a2)\n",
        "    a3 = np.dot(z2, W3) + b3\n",
        "\n",
        "    return a3\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "x = np.array([1.0, 0.5])\n",
        "y = forward(model, x)  # [ 0.31682708  0.69627909]\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.31682708 0.69627909]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVg7U_Ernu3f"
      },
      "source": [
        "## Pytorch\n",
        "아래는 pytorch를 이용해 구현한 코드이다.  \n",
        "주목해야할 부분은 torch의 dot product(행렬 곱)은 순서가 바꿔어서 진행이 된다. weight x input data 순서로 진행된다.  \n",
        "w1은 3x2행렬이고 입력값은 2x1 행렬이다. 둘을 행렬곱 하면 3x1행렬이 된다.  \n",
        "w2은 2x3행렬이고 w1의 출력값이 3x1행렬이기 때문에 행렬곱하면 2x1 행렬이 된다.  \n",
        "w3은 2x2행렬이고 w2의 출력값이 2x1행렬이기 때문에 행렬곱하면 2x1 행렬이 된다.  \n",
        "최종 아웃풋 값은 2x1 행렬을 transpose 값이 출력된다. \n",
        "\n",
        "numpy와 다르게 weight 값은 random하게 설정된다.  \n",
        "물론, 위의 코드처럼 초기 weight 값을 고정할 수 있지만 과정이 번거롭고 역전파 과정을 통해 결국은 weight가 조정되기 때문에 일단은 random하게 설정하는 것만 보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i8_5f54JaZH",
        "outputId": "a3323d7d-b4b3-4e07-c3b0-f16e2afce86a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FNN, self).__init__()\n",
        "\n",
        "    self.w1 = nn.Linear(2, 3, bias=True)\n",
        "    self.w2 = nn.Linear(3, 2, bias=True)\n",
        "    self.w3 = nn.Linear(2, 2, bias=True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # 내부 weight & bias 보기\n",
        "    print(\"weight_1 :\", self.w1.weight)\n",
        "    print(\"bias_1 :\", self.w1.bias)\n",
        "    print(self.w1.weight.shape, \"\\n\")\n",
        "    print(\"weight_2 :\", self.w2.weight)\n",
        "    print(\"bias_2 :\", self.w2.bias)\n",
        "    print(self.w2.weight.shape, \"\\n\")\n",
        "    print(\"weight_3 :\", self.w3.weight)\n",
        "    print(\"bias_3 :\", self.w3.bias)\n",
        "    print(self.w3.weight.shape, \"\\n\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.w1(x)\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.w2(x)\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.w3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "random_data = torch.FloatTensor([1, 0])\n",
        "model = FNN()\n",
        "print(\"output =>\", model(random_data))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight_1 : Parameter containing:\n",
            "tensor([[ 0.5113, -0.3429],\n",
            "        [-0.2401, -0.3754],\n",
            "        [ 0.0855,  0.7059]], requires_grad=True)\n",
            "bias_1 : Parameter containing:\n",
            "tensor([ 0.3335, -0.5134, -0.5670], requires_grad=True)\n",
            "torch.Size([3, 2]) \n",
            "\n",
            "weight_2 : Parameter containing:\n",
            "tensor([[-0.2759,  0.1872,  0.2063],\n",
            "        [-0.4060, -0.2080, -0.4843]], requires_grad=True)\n",
            "bias_2 : Parameter containing:\n",
            "tensor([-0.2462, -0.4592], requires_grad=True)\n",
            "torch.Size([2, 3]) \n",
            "\n",
            "weight_3 : Parameter containing:\n",
            "tensor([[-0.1803,  0.3744],\n",
            "        [ 0.6408,  0.1686]], requires_grad=True)\n",
            "bias_3 : Parameter containing:\n",
            "tensor([-0.2493,  0.5157], requires_grad=True)\n",
            "torch.Size([2, 2]) \n",
            "\n",
            "output => tensor([-0.2249,  0.8338], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}